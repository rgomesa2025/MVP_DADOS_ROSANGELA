{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc69d042-6ce8-4753-b4da-d409bf52bde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MVP - Construção de um pipeline de dados\n",
    "\n",
    "  A pipeline consistem em cinco etapas bem definidas, como buscar, coletar, modelar, carregar e análisar os dados.\n",
    "\n",
    "  Na primeira etapa que é a busca dos dados, foi escolhido dados abertos na web no site Portal da Transparencia do governo federal [https://portaldatransparencia.gov.br/].\n",
    "\n",
    "  Dentre as inumeras bases de dados no site, a escolhida foram os dados dos aposentados do Banco Central do Brasil (BACEN) que encontram-se ativos.  \n",
    "  O período de análise será de Janeiro de 2025 até Setembro de 2025.\n",
    "\n",
    "  A proposta é conseguir responder as seguintes perguntas com a massa de dados escolhida:\n",
    "  - Qual a faixa etária dos servidores?\n",
    "  - Qual é a média de idade desses servidores?   \n",
    "  - Quantos desses servidores estão ocupando cargos de alta gestão ou função comissionada? \n",
    "  - Qual foi o fluxo de aposentados na ativa no período? \n",
    "  - Quantos aposentados foram reativados a cada mês? \n",
    "  - Em quais departamentos a concentração de aposentados na ativa é maior? \n",
    "\n",
    "  A segunda etapa consistem em coletar esses dados do site e importar ele dentro da ferramaneta databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdcd234-fd46-4c35-8c7f-dfb2d3051e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "\n",
    "-- Cria o catálogo principal para todos os seus dados do BACEN\n",
    "CREATE CATALOG IF NOT EXISTS catalog_bacen\n",
    "COMMENT 'Catálogo para dados públicos dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- Opcional: Crie os esquemas (databases) Bronze, Silver e Gold dentro do novo catálogo\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.silver;\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.gold;\n",
    "\n",
    "-- 1. Criação do Volume para armazenar os arquivos\n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenará seus arquivos.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.bronze.dados_servidores\n",
    "COMMENT 'Volume para arquivos CSV brutos dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- 2. Criação do Volume para armazenar as tabelas \n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenará suas tabelas.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.silver.dados_servidores\n",
    "COMMENT 'Volume para armazenar as tabelas dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- 2. Criação do Volume para armazenar o modelo estrela\n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenará o modelo estrela.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.gold.dados_servidores\n",
    "COMMENT 'Volume para armazenar o modelo estrela dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- Define o catálogo recém-criado como o padrão para as próximas operações\n",
    "USE CATALOG catalog_bacen;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e38ac61-8812-416c-b19b-c502106b59ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tente rodar esta linha em uma célula separada antes do código principal\n",
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb915c76-ea07-4773-9afd-8ce4d7df99af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "# Importe a biblioteca unidecode\n",
    "from unidecode import unidecode \n",
    "from pyspark.sql.functions import col # Importar para renomear colunas no Spark\n",
    "\n",
    "# --- VARIÁVEIS DE CONFIGURAÇÃO ---\n",
    "ZIP_URL = \"https://portaldatransparencia.gov.br/download-de-dados/servidores/202501_Aposentados_BACEN/\"\n",
    "ZIP_FILENAME = \"202501_Aposentados_BACEN.zip\"\n",
    "\n",
    "# Caminho do Volume UC (Destino dos CSVs)\n",
    "UC_VOLUME_PATH = \"/Volumes/catalog_bacen/bronze/dados_servidores/\" \n",
    "\n",
    "# Diretórios Temporários no Driver\n",
    "TEMP_DOWNLOAD_PATH = f\"/tmp/{ZIP_FILENAME}\"\n",
    "TEMP_UNZIP_DIR = \"/tmp/unzip_servidores_data\"\n",
    "\n",
    "# --- FUNÇÃO DE LIMPEZA DE NOME DE COLUNA (ADICIONADA) ---\n",
    "def limpar_nome_coluna(nome_coluna):\n",
    "    \"\"\"\n",
    "    Função para normalizar e limpar nomes de colunas, convertendo-os para snake_case\n",
    "    e removendo caracteres inválidos para o formato Delta.\n",
    "    \"\"\"\n",
    "    # 1. Normaliza/Decodifica caracteres (remove acentos e corrige codificação)\n",
    "    nome_limpo = unidecode(nome_coluna) \n",
    "    \n",
    "    # 2. Remove o indicador de moeda e observação padronizando o final\n",
    "    # (R$)(*), (U$)(*), (R$), (U$) -> Substituímos por um marcador claro\n",
    "    nome_limpo = re.sub(r' \\(R\\$\\)(\\(\\*\\))?$', '_REAIS', nome_limpo)\n",
    "    nome_limpo = re.sub(r' \\(U\\$\\)(\\(\\*\\))?$', '_DOLAR', nome_limpo)\n",
    "    \n",
    "    # 3. Remove outros caracteres especiais e substitui por espaço\n",
    "    nome_limpo = re.sub(r'[/,;{}\\(\\)\\n\\t\\=–]', ' ', nome_limpo)\n",
    "    \n",
    "    # 4. Converte para snake_case (minúsculas, separadas por underscore)\n",
    "    nome_limpo = nome_limpo.strip().lower() \n",
    "    nome_limpo = re.sub(r'\\s+', '_', nome_limpo) # Substitui múltiplos espaços por um underscore\n",
    "    \n",
    "    # Remove underscores finais ou duplos que possam ter sobrado\n",
    "    nome_limpo = nome_limpo.strip('_')\n",
    "    nome_limpo = re.sub(r'__+', '_', nome_limpo)\n",
    "    \n",
    "    return nome_limpo\n",
    "\n",
    "# --- FLUXO DE TRABALHO CORRIGIDO E UNIFICADO ---\n",
    "print(f\"1. Iniciando o fluxo de ingestão...\")\n",
    "\n",
    "try:\n",
    "    # A. DOWNLOAD e DESCOMPACTAR (CÓDIGO INALTERADO)\n",
    "    print(f\"   -> Iniciando o download do arquivo ZIP: {ZIP_FILENAME}\")\n",
    "    response = requests.get(ZIP_URL, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(TEMP_DOWNLOAD_PATH, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "            file.write(chunk)\n",
    "            \n",
    "    os.makedirs(TEMP_UNZIP_DIR, exist_ok=True)\n",
    "    with ZipFile(TEMP_DOWNLOAD_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(TEMP_UNZIP_DIR)\n",
    "        \n",
    "    print(\"   -> Download e Descompactação concluídos.\")\n",
    "    \n",
    "    # B. INGESTÃO SEGURA: Iterar e Fazer o Upload de Cada CSV\n",
    "    print(f\"2. Movendo os arquivos CSV para o Volume UC: {UC_VOLUME_PATH}\")\n",
    "    \n",
    "    # Cria o diretório de destino no Volume (UC)\n",
    "    # dbutils.fs.mkdirs(UC_VOLUME_PATH) # Linha removida/comentada, pois o Spark fará isso.\n",
    "\n",
    "    moved_count = 0\n",
    "    \n",
    "    for filename in os.listdir(TEMP_UNZIP_DIR):\n",
    "        if filename.lower().endswith('.csv'):\n",
    "            local_csv_path = os.path.join(TEMP_UNZIP_DIR, filename)\n",
    "            \n",
    "            # O nome do arquivo Delta agora usará o nome limpo do CSV\n",
    "            remote_delta_path = UC_VOLUME_PATH + filename.replace(\".csv\", \"\") + \"_delta\"\n",
    "            \n",
    "            print(f\"   -> Processando: {filename}\")\n",
    "            \n",
    "            # 1. Lê o conteúdo do arquivo CSV local como texto\n",
    "            with open(local_csv_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                csv_content = f.read()\n",
    "\n",
    "            # 2. Usa dbutils.fs.put para escrever o conteúdo como um arquivo temporário no Volume\n",
    "            temp_csv_file_on_volume = f\"{UC_VOLUME_PATH}temp_{filename}\"\n",
    "            dbutils.fs.put(temp_csv_file_on_volume, csv_content, overwrite=True)\n",
    "            \n",
    "            # 3. Usa Spark para ler o CSV do Volume\n",
    "            df_temp = (spark.read \n",
    "                       .format(\"csv\") \n",
    "                       .option(\"header\", \"true\") \n",
    "                       .option(\"delimiter\", \";\") \n",
    "                       .option(\"encoding\", \"ISO-8859-1\") \n",
    "                       .load(temp_csv_file_on_volume)\n",
    "                      )\n",
    "            \n",
    "            # 4. APLICAÇÃO DA LIMPEZA DOS NOMES DAS COLUNAS (NOVO PASSO CRÍTICO)\n",
    "            print(\"     -> Limpando nomes de colunas...\")\n",
    "            \n",
    "            # Cria um dicionário de mapeamento: 'Nome Antigo' -> 'nome_novo_limpo'\n",
    "            mapeamento_colunas = {\n",
    "                c: limpar_nome_coluna(c) \n",
    "                for c in df_temp.columns\n",
    "            }\n",
    "            \n",
    "            # Aplica a renomeação usando o Spark (mais eficiente em grandes DataFrames)\n",
    "            df_limpo = df_temp\n",
    "            for old_name, new_name in mapeamento_colunas.items():\n",
    "                df_limpo = df_limpo.withColumnRenamed(old_name, new_name)\n",
    "                \n",
    "            # print(\"Exemplo de colunas após limpeza:\", df_limpo.columns[:5])\n",
    "            \n",
    "            # 5. Salva o DataFrame LIMPO como Tabela Delta (Formato Bronze)\n",
    "            print(\"     -> Salvando como formato Delta...\")\n",
    "            df_limpo.write.format(\"delta\").mode(\"overwrite\").save(remote_delta_path)\n",
    "            \n",
    "            # 6. Remove o arquivo CSV temporário no Volume\n",
    "            dbutils.fs.rm(temp_csv_file_on_volume)\n",
    "            \n",
    "            moved_count += 1\n",
    "    \n",
    "    # ... (Restante do bloco de conclusão inalterado)\n",
    "    if moved_count > 0:\n",
    "        print(f\"\\n   -> {moved_count} arquivo(s) CSV lido(s) localmente, colunas limpas e salvo(s) como Delta na Camada Bronze com sucesso!\")\n",
    "    else:\n",
    "        print(\"\\n   -> ATENÇÃO: Nenhum arquivo CSV encontrado na pasta descompactada.\")\n",
    "\n",
    "\n",
    "    # C. LIMPEZA (CÓDIGO INALTERADO)\n",
    "    print(\"3. Limpeza de arquivos temporários...\")\n",
    "    \n",
    "    os.remove(TEMP_DOWNLOAD_PATH) \n",
    "    shutil.rmtree(TEMP_UNZIP_DIR) \n",
    "    \n",
    "    print(\"   -> Limpeza concluída.\")\n",
    "    \n",
    "    # D. CONFIRMAÇÃO (CÓDIGO INALTERADO)\n",
    "    print(\"\\nConteúdo do Volume de destino (deve ser o arquivo delta):\")\n",
    "    dbutils.fs.ls(UC_VOLUME_PATH)\n",
    "\n",
    "# --- TRATAMENTO DE ERROS UNIFICADO (CÓDIGO INALTERADO) ---\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nERRO DE DOWNLOAD: Falha ao acessar a URL. Verifique se a URL está correta. Detalhes: {e}\")\n",
    "except Exception as e:\n",
    "    # Este catch agora trata qualquer erro durante a descompactação ou ingestão\n",
    "    print(f\"\\nOCORREU UM ERRO INESPERADO: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6574085027863048,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MVP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
