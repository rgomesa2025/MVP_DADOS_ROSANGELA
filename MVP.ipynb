{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc69d042-6ce8-4753-b4da-d409bf52bde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MVP - Constru√ß√£o de um pipeline de dados\n",
    "\n",
    "  A pipeline consistem em cinco etapas bem definidas, como buscar, coletar, modelar, carregar e an√°lisar os dados.\n",
    "\n",
    "  Na primeira etapa que √© a busca dos dados, foi escolhido dados abertos na web no site Portal da Transparencia do governo federal [https://portaldatransparencia.gov.br/].\n",
    "\n",
    "  Dentre as inumeras bases de dados no site, a escolhida foram os dados dos aposentados do Banco Central do Brasil (BACEN) que encontram-se ativos.  \n",
    "  O per√≠odo de an√°lise ser√° de Janeiro de 2025 at√© Setembro de 2025.\n",
    "\n",
    "  A proposta √© conseguir responder as seguintes perguntas com a massa de dados escolhida:\n",
    "  - Qual a faixa et√°ria dos servidores?\n",
    "  - Qual √© a m√©dia de idade desses servidores?   \n",
    "  - Quantos desses servidores est√£o ocupando cargos de alta gest√£o ou fun√ß√£o comissionada? \n",
    "  - Qual foi o fluxo de aposentados na ativa no per√≠odo? \n",
    "  - Quantos aposentados foram reativados a cada m√™s? \n",
    "  - Em quais departamentos a concentra√ß√£o de aposentados na ativa √© maior? \n",
    "\n",
    "  A segunda etapa consistem em coletar esses dados do site e importar ele dentro da ferramaneta databricks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e63bf4-d4bc-421b-bb04-b4eb87969aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parapara√ß√£o para a camada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdcd234-fd46-4c35-8c7f-dfb2d3051e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "\n",
    "-- Cria o cat√°logo principal para todos os seus dados do BACEN\n",
    "CREATE CATALOG IF NOT EXISTS catalog_bacen\n",
    "COMMENT 'Cat√°logo para dados p√∫blicos dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- Opcional: Crie os esquemas (databases) Bronze, Silver e Gold dentro do novo cat√°logo\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.silver;\n",
    "CREATE SCHEMA IF NOT EXISTS catalog_bacen.gold;\n",
    "\n",
    "-- 1. Cria√ß√£o do Volume para armazenar os arquivos\n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenar√° seus arquivos.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.bronze.dados_servidores\n",
    "COMMENT 'Volume para arquivos CSV brutos dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- 2. Cria√ß√£o do Volume para armazenar as tabelas \n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenar√° suas tabelas.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.silver.dados_servidores\n",
    "COMMENT 'Volume para armazenar as tabelas dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- 2. Cria√ß√£o do Volume para armazenar o modelo estrela\n",
    "-- Assumimos o nome 'dados_servidores' para o Volume que armazenar√° o modelo estrela.\n",
    "CREATE VOLUME IF NOT EXISTS catalog_bacen.gold.dados_servidores\n",
    "COMMENT 'Volume para armazenar o modelo estrela dos Servidores Aposentados Ativos do Banco do Brasil (BACEN).';\n",
    "\n",
    "-- Define o cat√°logo rec√©m-criado como o padr√£o para as pr√≥ximas opera√ß√µes\n",
    "USE CATALOG catalog_bacen;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03767b09-d83e-4d10-a9ee-8cbde2ff620c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Camada Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e38ac61-8812-416c-b19b-c502106b59ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\nInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.4.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Tente rodar esta linha em uma c√©lula separada antes do c√≥digo principal\n",
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb915c76-ea07-4773-9afd-8ce4d7df99af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processamento para os meses: ['202501', '202502', '202503', '202504', '202505', '202506', '202507', '202508', '202509']\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202501\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202501_Aposentados_BACEN/\n    -> Lendo CSV: 202501_Remuneracao.csv\nWrote 1722463 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202501_Remuneracao_delta\n    -> Lendo CSV: 202501_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202501_Observacoes_delta\n    -> Lendo CSV: 202501_Cadastro.csv\nWrote 1779125 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202501_Cadastro_delta\n -> Conclu√≠do com sucesso para 202501\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202502\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202502_Aposentados_BACEN/\n    -> Lendo CSV: 202502_Cadastro.csv\nWrote 1777220 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202502_Cadastro_delta\n    -> Lendo CSV: 202502_Remuneracao.csv\nWrote 1720778 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202502_Remuneracao_delta\n    -> Lendo CSV: 202502_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202502_Observacoes_delta\n -> Conclu√≠do com sucesso para 202502\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202503\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202503_Aposentados_BACEN/\n    -> Lendo CSV: 202503_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202503_Observacoes_delta\n    -> Lendo CSV: 202503_Remuneracao.csv\nWrote 1720047 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202503_Remuneracao_delta\n    -> Lendo CSV: 202503_Cadastro.csv\nWrote 6867003 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202503_Cadastro_delta\n -> Conclu√≠do com sucesso para 202503\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202504\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202504_Aposentados_BACEN/\n    -> Lendo CSV: 202504_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202504_Observacoes_delta\n    -> Lendo CSV: 202504_Cadastro.csv\nWrote 6867235 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202504_Cadastro_delta\n    -> Lendo CSV: 202504_Remuneracao.csv\nWrote 1757014 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202504_Remuneracao_delta\n -> Conclu√≠do com sucesso para 202504\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202505\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202505_Aposentados_BACEN/\n    -> Lendo CSV: 202505_Cadastro.csv\nWrote 1918853 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202505_Cadastro_delta\n    -> Lendo CSV: 202505_Remuneracao.csv\nWrote 1722019 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202505_Remuneracao_delta\n    -> Lendo CSV: 202505_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202505_Observacoes_delta\n -> Conclu√≠do com sucesso para 202505\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202506\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202506_Aposentados_BACEN/\n    -> Lendo CSV: 202506_Remuneracao.csv\nWrote 1719758 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202506_Remuneracao_delta\n    -> Lendo CSV: 202506_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202506_Observacoes_delta\n    -> Lendo CSV: 202506_Cadastro.csv\nWrote 1916333 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202506_Cadastro_delta\n -> Conclu√≠do com sucesso para 202506\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202507\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202507_Aposentados_BACEN/\n    -> Lendo CSV: 202507_Cadastro.csv\nWrote 1915957 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202507_Cadastro_delta\n    -> Lendo CSV: 202507_Remuneracao.csv\nWrote 1719340 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202507_Remuneracao_delta\n    -> Lendo CSV: 202507_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202507_Observacoes_delta\n -> Conclu√≠do com sucesso para 202507\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202508\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202508_Aposentados_BACEN/\n    -> Lendo CSV: 202508_Remuneracao.csv\nWrote 1719621 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202508_Remuneracao_delta\n    -> Lendo CSV: 202508_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202508_Observacoes_delta\n    -> Lendo CSV: 202508_Cadastro.csv\nWrote 1915963 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202508_Cadastro_delta\n -> Conclu√≠do com sucesso para 202508\n\n==================================================\nPROCESSANDO COMPET√äNCIA: 202509\n==================================================\n -> Baixando: https://portaldatransparencia.gov.br/download-de-dados/servidores/202509_Aposentados_BACEN/\n    -> Lendo CSV: 202509_Cadastro.csv\nWrote 1914508 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202509_Cadastro_delta\n    -> Lendo CSV: 202509_Observacoes.csv\nWrote 59 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202509_Observacoes_delta\n    -> Lendo CSV: 202509_Remuneracao.csv\nWrote 1718299 bytes.\n    -> Sucesso: /Volumes/catalog_bacen/bronze/dados_servidores/202509_Remuneracao_delta\n -> Conclu√≠do com sucesso para 202509\n\nPipeline finalizado para todos os meses dispon√≠veis.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE CAMINHOS ---\n",
    "UC_VOLUME_PATH = \"/Volumes/catalog_bacen/bronze/dados_servidores/\"\n",
    "\n",
    "# --- FUN√á√ÉO DE LIMPEZA (MANTIDA) ---\n",
    "def limpar_nome_coluna(nome_coluna):\n",
    "    nome_limpo = unidecode(nome_coluna) \n",
    "    nome_limpo = re.sub(r' \\(R\\$\\)(\\(\\*\\))?$', '_REAIS', nome_limpo)\n",
    "    nome_limpo = re.sub(r' \\(U\\$\\)(\\(\\*\\))?$', '_DOLAR', nome_limpo)\n",
    "    nome_limpo = re.sub(r'[/,;{}\\(\\)\\n\\t\\=‚Äì]', ' ', nome_limpo)\n",
    "    nome_limpo = nome_limpo.strip().lower() \n",
    "    nome_limpo = re.sub(r'\\s+', '_', nome_limpo)\n",
    "    nome_limpo = nome_limpo.strip('_')\n",
    "    nome_limpo = re.sub(r'__+', '_', nome_limpo)\n",
    "    return nome_limpo\n",
    "\n",
    "# --- LOOP PARA OS MESES (JANEIRO A SETEMBRO) ---\n",
    "# Geramos uma lista de meses formatados: ['202501', '202502', ..., '202509']\n",
    "meses_para_processar = [f\"2025{str(mes).zfill(2)}\" for mes in range(1, 10)]\n",
    "\n",
    "print(f\"Iniciando processamento para os meses: {meses_para_processar}\")\n",
    "\n",
    "for competencia in meses_para_processar:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"PROCESSANDO COMPET√äNCIA: {competencia}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Dinamiza as vari√°veis com base no m√™s\n",
    "    ZIP_URL = f\"https://portaldatransparencia.gov.br/download-de-dados/servidores/{competencia}_Aposentados_BACEN/\"\n",
    "    ZIP_FILENAME = f\"{competencia}_Aposentados_BACEN.zip\"\n",
    "    TEMP_DOWNLOAD_PATH = f\"/tmp/{ZIP_FILENAME}\"\n",
    "    TEMP_UNZIP_DIR = f\"/tmp/unzip_{competencia}\"\n",
    "\n",
    "    try:\n",
    "        # A. DOWNLOAD\n",
    "        print(f\" -> Baixando: {ZIP_URL}\")\n",
    "        response = requests.get(ZIP_URL, stream=True)\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            print(f\" [AVISO] Dados para {competencia} n√£o encontrados (404). Pulando...\")\n",
    "            continue\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(TEMP_DOWNLOAD_PATH, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                file.write(chunk)\n",
    "                \n",
    "        # B. DESCOMPACTAR\n",
    "        os.makedirs(TEMP_UNZIP_DIR, exist_ok=True)\n",
    "        with ZipFile(TEMP_DOWNLOAD_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(TEMP_UNZIP_DIR)\n",
    "        \n",
    "        # C. PROCESSAR ARQUIVOS\n",
    "        for filename in os.listdir(TEMP_UNZIP_DIR):\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                local_csv_path = os.path.join(TEMP_UNZIP_DIR, filename)\n",
    "                remote_delta_path = UC_VOLUME_PATH + filename.replace(\".csv\", \"\") + \"_delta\"\n",
    "                \n",
    "                print(f\"    -> Lendo CSV: {filename}\")\n",
    "                \n",
    "                # Lendo com tratamento de encoding\n",
    "                with open(local_csv_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                    csv_content = f.read()\n",
    "\n",
    "                temp_csv_file_on_volume = f\"{UC_VOLUME_PATH}temp_{competencia}_{filename}\"\n",
    "                dbutils.fs.put(temp_csv_file_on_volume, csv_content, overwrite=True)\n",
    "                \n",
    "                # Spark Read\n",
    "                df_temp = (spark.read \n",
    "                           .format(\"csv\") \n",
    "                           .option(\"header\", \"true\") \n",
    "                           .option(\"delimiter\", \";\") \n",
    "                           .option(\"encoding\", \"ISO-8859-1\") \n",
    "                           .load(temp_csv_file_on_volume)\n",
    "                          )\n",
    "                \n",
    "                # Renomear Colunas\n",
    "                mapeamento = {c: limpar_nome_coluna(c) for c in df_temp.columns}\n",
    "                df_limpo = df_temp\n",
    "                for old_name, new_name in mapeamento.items():\n",
    "                    df_limpo = df_limpo.withColumnRenamed(old_name, new_name)\n",
    "                \n",
    "                # Salvar Delta\n",
    "                df_limpo.write.format(\"delta\").mode(\"overwrite\").save(remote_delta_path)\n",
    "                dbutils.fs.rm(temp_csv_file_on_volume)\n",
    "                print(f\"    -> Sucesso: {remote_delta_path}\")\n",
    "\n",
    "        # D. LIMPEZA TEMPOR√ÅRIA (por m√™s)\n",
    "        os.remove(TEMP_DOWNLOAD_PATH) \n",
    "        shutil.rmtree(TEMP_UNZIP_DIR)\n",
    "        print(f\" -> Conclu√≠do com sucesso para {competencia}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" [ERRO] Falha ao processar {competencia}: {e}\")\n",
    "        # Continua para o pr√≥ximo m√™s mesmo se um falhar\n",
    "        continue\n",
    "\n",
    "print(\"\\nPipeline finalizado para todos os meses dispon√≠veis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "877354a4-02a6-4770-9df1-011e34463593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "798d815b-63a1-4f8b-83f5-c55b139c0031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Localizando tabelas na Bronze...\n   -> Encontradas 27 pastas. Carregando e unificando...\nüîç Colunas detectadas: ['id_servidor_portal', 'nome', 'cpf', 'matricula', 'cod_tipo_aposentadoria', 'tipo_aposentadoria', 'data_aposentadoria', 'descricao_cargo', 'cod_uorg_lotacao', 'uorg_lotacao', 'cod_org_lotacao', 'org_lotacao', 'cod_orgsup_lotacao', 'orgsup_lotacao', 'cod_tipo_vinculo', 'tipo_vinculo', 'situacao_vinculo', 'regime_juridico', 'jornada_de_trabalho', 'data_ingresso_cargofuncao', 'data_nomeacao_cargofuncao', 'data_ingresso_orgao', 'documento_ingresso_servicopublico', 'data_diploma_ingresso_servicopublico', 'diploma_ingresso_cargofuncao', 'diploma_ingresso_orgao', 'diploma_ingresso_servicopublico', 'ano', 'mes', 'observacao', 'remuneraaao_basica_bruta_reais', 'remuneraaao_basica_bruta_dolar', 'abate-teto_reais', 'abate-teto_dolar', 'gratificaaao_natalina_reais', 'gratificaaao_natalina_dolar', 'abate-teto_da_gratificaaao_natalina_reais', 'abate-teto_da_gratificaaao_natalina_dolar', 'farias_reais', 'farias_dolar', 'outras_remuneraaaes_eventuais_reais', 'outras_remuneraaaes_eventuais_dolar', 'irrf_reais', 'irrf_dolar', 'pss_rpgs_reais', 'pss_rpgs_dolar', 'demais_deduaaes_reais', 'demais_deduaaes_dolar', 'pensao_militar_reais', 'pensao_militar_dolar', 'fundo_de_saade_reais', 'fundo_de_saade_dolar', 'taxa_de_ocupaaao_imavel_funcional_reais', 'taxa_de_ocupaaao_imavel_funcional_dolar', 'remuneraaao_apas_deduaaes_obrigatarias_reais', 'remuneraaao_apas_deduaaes_obrigatarias_dolar', 'verbas_indenizatarias_registradas_em_sistemas_de_pessoal_-_civil_reais', 'verbas_indenizatarias_registradas_em_sistemas_de_pessoal_-_civil_dolar', 'verbas_indenizatarias_registradas_em_sistemas_de_pessoal_-_militar_reais', 'verbas_indenizatarias_registradas_em_sistemas_de_pessoal_-_militar_dolar', 'verbas_indenizatarias_programa_desligamento_voluntario_a_mp_792_2017_reais', 'verbas_indenizatarias_programa_desligamento_voluntario_a_mp_792_2017_dolar', 'total_de_verbas_indenizatarias_reais', 'total_de_verbas_indenizatarias_dolar']\n   -> Usando a coluna 'mes' para gerar a data de refer√™ncia.\n2. Iniciando limpeza financeira...\n3. Tratando campos de data e auditoria...\n4. Gravando camada Silver consolidada em: /Volumes/catalog_bacen/silver/dados_servidores\n‚úÖ Camada Silver pronta e validada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, current_timestamp, lit, concat, lpad\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE CAMINHOS ---\n",
    "UC_VOLUME_PATH = \"/Volumes/catalog_bacen/bronze/dados_servidores/\"\n",
    "PATH_SILVER = \"/Volumes/catalog_bacen/silver/dados_servidores\"\n",
    "\n",
    "print(f\"1. Localizando tabelas na Bronze...\")\n",
    "\n",
    "# Lista todos os arquivos/pastas no volume\n",
    "files = dbutils.fs.ls(UC_VOLUME_PATH)\n",
    "\n",
    "# Filtra apenas as pastas que terminam com '_delta'\n",
    "paths_to_load = [f.path for f in files if f.path.endswith(\"_delta/\") or \"_delta\" in f.path]\n",
    "\n",
    "if not paths_to_load:\n",
    "    raise Exception(f\"Nenhuma tabela Delta encontrada em {UC_VOLUME_PATH}. Verifique se a camada Bronze rodou com sucesso.\")\n",
    "\n",
    "print(f\"   -> Encontradas {len(paths_to_load)} pastas. Carregando e unificando...\")\n",
    "\n",
    "# L√™ e unifica as tabelas\n",
    "df_bronze = spark.read.format(\"delta\").load(paths_to_load[0])\n",
    "\n",
    "for path in paths_to_load[1:]:\n",
    "    df_proximo = spark.read.format(\"delta\").load(path)\n",
    "    df_bronze = df_bronze.unionByName(df_proximo, allowMissingColumns=True)\n",
    "\n",
    "# --- 2. TRATAMENTO DIN√ÇMICO DE COLUNAS ---\n",
    "colunas_disponiveis = df_bronze.columns\n",
    "print(f\"üîç Colunas detectadas: {colunas_disponiveis}\")\n",
    "\n",
    "# Identifica qual coluna de data usar\n",
    "col_data_origem = None\n",
    "for candidata in [\"mes_referencia\", \"mes\", \"MES_REFERENCIA\", \"MES\"]:\n",
    "    if candidata in colunas_disponiveis:\n",
    "        col_data_origem = candidata\n",
    "        break\n",
    "\n",
    "if not col_data_origem:\n",
    "    raise Exception(\"N√£o foi poss√≠vel encontrar uma coluna de data (mes ou mes_referencia) nos dados.\")\n",
    "\n",
    "print(f\"   -> Usando a coluna '{col_data_origem}' para gerar a data de refer√™ncia.\")\n",
    "\n",
    "# --- 3. TRANSFORMA√á√ïES (LIMPEZA FINANCEIRA) ---\n",
    "print(\"2. Iniciando limpeza financeira...\")\n",
    "cols_financeiras = [c for c in df_bronze.columns if '_reais' in c or '_dolar' in c]\n",
    "\n",
    "df_silver = df_bronze\n",
    "\n",
    "for c in cols_financeiras:\n",
    "    # Tratamento para converter \"1.234,56\" em 1234.56 (Double)\n",
    "    df_silver = df_silver.withColumn(\n",
    "        c, \n",
    "        F.regexp_replace(F.regexp_replace(F.col(c), r'\\.', ''), ',', '.').cast(\"double\")\n",
    "    )\n",
    "\n",
    "# --- 4. DATA E AUDITORIA (CORRE√á√ÉO DO CANNOT_PARSE_TIMESTAMP) ---\n",
    "print(\"3. Tratando campos de data e auditoria...\")\n",
    "\n",
    "# 1. lpad garante que o m√™s tenha 2 d√≠gitos (ex: '2' vira '02')\n",
    "# 2. Concatena '01/' + m√™s + '/' + ano (coluna detectada no seu print)\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"data_referencia\", \n",
    "    F.to_date(\n",
    "        F.concat(\n",
    "            F.lit(\"01/\"), \n",
    "            F.lpad(F.col(col_data_origem), 2, \"0\"), \n",
    "            F.lit(\"/\"), \n",
    "            F.col(\"ano\")\n",
    "        ), \n",
    "        \"dd/MM/yyyy\"\n",
    "    )\n",
    ").withColumn(\"etl_load_timestamp\", F.current_timestamp())\n",
    "\n",
    "# --- 5. GRAVA√á√ÉO ---\n",
    "print(f\"4. Gravando camada Silver consolidada em: {PATH_SILVER}\")\n",
    "\n",
    "# Criamos o diret√≥rio silver se n√£o existir\n",
    "dbutils.fs.mkdirs(\"/Volumes/catalog_bacen/silver/dados_servidores/\")\n",
    "\n",
    "# Grava√ß√£o com overwriteSchema\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(PATH_SILVER)\n",
    "\n",
    "print(\"‚úÖ Camada Silver pronta e validada com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6574085027863048,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MVP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
